{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "from sklearn.metrics import roc_auc_score, log_loss, root_mean_squared_error\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, hp, tpe, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from misc import cfg, supports, drift_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CONFIG:\n",
      "---mode > test\n",
      "---mlflow_evals_nbr > 2\n",
      "---booster_rounds > 10\n",
      "---skip_optimization > False\n"
     ]
    }
   ],
   "source": [
    "# initialize configuration\n",
    "# mode=test will minimize parameters (ex: 20 VS 1000 booster runs)\n",
    "# skip_optimization=True will skip model tuning\n",
    "CONFIG = cfg.init_config(mode=\"test\", skip_optimization=False)\n",
    "\n",
    "# output config\n",
    "print(\"---CONFIG:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"---{k} > {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/27 14:46:05 INFO mlflow.tracking.fluent: Experiment with name 'political_engagement' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/andre/political-engagement-mlops/mlruns/1', creation_time=1727441165304, experiment_id='1', last_update_time=1727441165304, lifecycle_stage='active', name='political_engagement', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # launch mlflow\n",
    "# mlflow ui --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow/mlflow.db\")\n",
    "mlflow.set_experiment(\"political_engagement\")\n",
    "# # enable auto-log\n",
    "# mlflow.xgboost.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = drift_handler.get_data(\"./data/training_data/production_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training context\n",
    "# get datasets\n",
    "dftrain, dftest = train_test_split(data, test_size=0.2, random_state=99)\n",
    "\n",
    "# get targets\n",
    "ytrain = dftrain[\"political_engagement\"].values\n",
    "ytest = dftest[\"political_engagement\"].values\n",
    "dftrain.drop(columns=[\"political_engagement\"], inplace=True)\n",
    "dftest.drop(columns=[\"political_engagement\"], inplace=True)\n",
    "\n",
    "# vectorize\n",
    "dv = DictVectorizer(sparse=False)\n",
    "train_dict = dftrain.to_dict(orient=\"records\")\n",
    "test_dict = dftest.to_dict(orient=\"records\")\n",
    "xtrain = dv.fit_transform(train_dict)\n",
    "xtest = dv.transform(test_dict)\n",
    "feature_names = dv.get_feature_names_out().tolist()\n",
    "\n",
    "# get dmatrix\n",
    "xtrain = xgb.DMatrix(xtrain, label=ytrain)\n",
    "xtest = xgb.DMatrix(xtest, label=ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:13.59317                            \n",
      "[1]\ttest-logloss:22.53201                            \n",
      "[2]\ttest-logloss:25.35753                            \n",
      "[3]\ttest-logloss:25.35753                            \n",
      "[4]\ttest-logloss:25.35753                            \n",
      "[5]\ttest-logloss:25.35753                            \n",
      "[6]\ttest-logloss:25.35753                            \n",
      "[7]\ttest-logloss:25.35753                            \n",
      "[8]\ttest-logloss:25.35753                            \n",
      "[9]\ttest-logloss:25.35753                            \n",
      "[0]\ttest-logloss:1.40450                                                       \n",
      "[1]\ttest-logloss:1.40412                                                       \n",
      "[2]\ttest-logloss:1.40375                                                       \n",
      "[3]\ttest-logloss:1.40337                                                       \n",
      "[4]\ttest-logloss:1.40301                                                       \n",
      "[5]\ttest-logloss:1.40264                                                       \n",
      "[6]\ttest-logloss:1.40229                                                       \n",
      "[7]\ttest-logloss:1.40193                                                       \n",
      "[8]\ttest-logloss:1.40158                                                       \n",
      "[9]\ttest-logloss:1.40123                                                       \n",
      "100%|██████████| 2/2 [00:04<00:00,  2.17s/trial, best loss: 1.4012346909562237]\n"
     ]
    }
   ],
   "source": [
    "# optimize\n",
    "if not CONFIG[\"skip_optimization\"]:\n",
    "    search_space = {\n",
    "        \"learning_rate\": hp.loguniform(\"learning_rate\", -7, 10),\n",
    "        \"max_depth\": scope.int(hp.quniform(\"max_depth\", 0, 100, 1)),\n",
    "        \"min_child_weight\": hp.loguniform(\"min_child_weight\", -1, 4.6),\n",
    "        \"reg_alpha\": hp.loguniform(\"reg_alpha\", -5, 4.6), \n",
    "        \"scale_pos_weight\": hp.loguniform(\"scale_pos_weight\", 0, 4.6),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"seed\": 99\n",
    "    }\n",
    "\n",
    "    best_result = fmin(\n",
    "        fn=lambda search_space: supports.objective(\n",
    "            search_space=search_space,\n",
    "            xtrain=xtrain,\n",
    "            xtest=xtest,\n",
    "            ytrain=ytrain,\n",
    "            ytest=ytest,\n",
    "            num_boost_round=CONFIG[\"booster_rounds\"]\n",
    "            ),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=CONFIG[\"mlflow_evals_nbr\"],\n",
    "        trials=Trials()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:1.40450\n",
      "[1]\ttest-logloss:1.40412\n",
      "[2]\ttest-logloss:1.40375\n",
      "[3]\ttest-logloss:1.40337\n",
      "[4]\ttest-logloss:1.40301\n",
      "[5]\ttest-logloss:1.40264\n",
      "[6]\ttest-logloss:1.40229\n",
      "[7]\ttest-logloss:1.40193\n",
      "[8]\ttest-logloss:1.40158\n",
      "[9]\ttest-logloss:1.40123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/miniconda3/envs/mlops/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [14:47:02] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.4012346909562237, 'status': 'ok'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model with the best params\n",
    "artifacts_path = \"./mlflow\"\n",
    "tags = {\n",
    "        \"author\": \"andrei lupascu\",\n",
    "        \"mode\": CONFIG[\"mode\"]\n",
    "}\n",
    "# update best_result\n",
    "best_result[\"objective\"] = search_space[\"objective\"]\n",
    "best_result[\"seed\"] = search_space[\"seed\"]\n",
    "# format best results (some params need to be cast as int)\n",
    "int_params = [\"max_depth\"]\n",
    "for int_param in int_params:\n",
    "        best_result[int_param] = int(best_result[int_param])\n",
    "\n",
    "supports.objective(\n",
    "     search_space=best_result,\n",
    "     xtrain=xtrain,\n",
    "     xtest=xtest,\n",
    "     ytrain=ytrain,\n",
    "     ytest=ytest,\n",
    "     num_boost_round=CONFIG[\"booster_rounds\"],\n",
    "     tags=tags,\n",
    "     save_artifacts=(True, artifacts_path, dv)\n",
    "     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:1.40450\n",
      "[1]\ttest-logloss:1.40412\n",
      "[2]\ttest-logloss:1.40375\n",
      "[3]\ttest-logloss:1.40337\n",
      "[4]\ttest-logloss:1.40301\n",
      "[5]\ttest-logloss:1.40264\n",
      "[6]\ttest-logloss:1.40229\n",
      "[7]\ttest-logloss:1.40193\n",
      "[8]\ttest-logloss:1.40158\n",
      "[9]\ttest-logloss:1.40123\n"
     ]
    }
   ],
   "source": [
    "int_params = [\"max_depth\"]\n",
    "for int_param in int_params:\n",
    "        best_result[int_param] = int(best_result[int_param])\n",
    "\n",
    "booster = xgb.train(\n",
    "params=best_result,\n",
    "dtrain=xtrain,\n",
    "num_boost_round=CONFIG[\"booster_rounds\"],\n",
    "evals=[(xtest, \"test\")],\n",
    "early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     mlflow.set_tags(tags)\n",
    "#     # train model\n",
    "#     booster = xgb.train(\n",
    "#     params=best_result,\n",
    "#     dtrain=xtrain,\n",
    "#     num_boost_round=CONFIG[\"booster_rounds\"],\n",
    "#     evals=[(xtest, \"test\")],\n",
    "#     early_stopping_rounds=50\n",
    "#     )\n",
    "\n",
    "#     # get metrics\n",
    "#     ytrain_pred = booster.predict(xtrain)\n",
    "#     yval_pred = booster.predict(xtest)\n",
    "#     # get auc\n",
    "#     train_auc = roc_auc_score(ytrain, ytrain_pred)\n",
    "#     test_auc = roc_auc_score(ytest, yval_pred)\n",
    "#     # get loss\n",
    "#     train_log_loss = log_loss(ytrain, ytrain_pred)\n",
    "#     test_log_loss = log_loss(ytest, yval_pred)\n",
    "#     # get rmse\n",
    "#     train_rmse = root_mean_squared_error(ytrain, ytrain_pred)\n",
    "#     test_rmse = root_mean_squared_error(ytest, yval_pred)\n",
    "#     # store metrics\n",
    "#     metrics = {\n",
    "#             \"train_auc\": train_auc,\n",
    "#             \"test_auc\": test_auc,\n",
    "#             \"train_log_loss\": train_log_loss,\n",
    "#             \"test_log_loss\": test_log_loss,\n",
    "#             \"train_rmse\": train_rmse,\n",
    "#             \"test_rmse\": test_rmse\n",
    "#     }\n",
    "\n",
    "#         # log metrics\n",
    "#     for name, metric in metrics.items():\n",
    "#         mlflow.log_metric(name, metric)\n",
    "\n",
    "#     # log artifacts  \n",
    "#     with open(f\"./mlflow/{preprocessor_name}.bin\", \"wb\") as fout:\n",
    "#             pickle.dump(dv, fout)\n",
    "#     booster.save_model(f\"./mlflow/{model_name}.xgb\")\n",
    "#     mlflow.log_artifact(f\"./mlflow/{preprocessor_name}.bin\", artifact_path=preprocessor_name)\n",
    "#     mlflow.log_artifact(f\"./mlflow/{model_name}.xgb\", artifact_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_params = [\"max_depth\"]\n",
    "# for int_param in int_params:\n",
    "#     best_result[int_param] = int(best_result[int_param])\n",
    "    \n",
    "# with mlflow.start_run():\n",
    "#     # train model\n",
    "#     model = xgb.train(\n",
    "#     params=best_result,\n",
    "#     dtrain=xtrain,\n",
    "#     num_boost_round=CONFIG[\"booster_rounds\"],\n",
    "#     evals=[(xtest, \"test\")],\n",
    "#     early_stopping_rounds=50\n",
    "#     )\n",
    "\n",
    "#     # get metrics\n",
    "#     ytrain_pred = booster.predict(xtrain)\n",
    "#     yval_pred = booster.predict(xtest)\n",
    "#     # get auc\n",
    "#     train_auc = roc_auc_score(ytrain, ytrain_pred)\n",
    "#     test_auc = roc_auc_score(ytest, yval_pred)\n",
    "#     # get loss\n",
    "#     train_log_loss = log_loss(ytrain, ytrain_pred)\n",
    "#     test_log_loss = log_loss(ytest, yval_pred)\n",
    "#     # get rmse\n",
    "#     train_rmse = root_mean_squared_error(ytrain, ytrain_pred)\n",
    "#     test_rmse = root_mean_squared_error(ytest, yval_pred)\n",
    "#     # store metrics\n",
    "#     metrics = {\n",
    "#             \"train_auc\": train_auc,\n",
    "#             \"test_auc\": test_auc,\n",
    "#             \"train_log_loss\": train_log_loss,\n",
    "#             \"test_log_loss\": test_log_loss,\n",
    "#             \"train_rmse\": train_rmse,\n",
    "#             \"test_rmse\": test_rmse\n",
    "#     }\n",
    "\n",
    "#         # log metrics\n",
    "#     for name, metric in metrics.items():\n",
    "#         mlflow.log_metric(name, metric)\n",
    "\n",
    "#     # log artifacts  \n",
    "#     with open(f\"./mlflow/DV.bin\", \"wb\") as fout:\n",
    "#             pickle.dump(dv, fout)\n",
    "#     booster.save_model(\"./mlflow/MODEL.xgb\")\n",
    "#     mlflow.log_artifact(\"./mlflow/DV.bin\", artifact_path=\"DV\")\n",
    "#     mlflow.log_artifact(\"./mlflow/MODEL.xgb\", artifact_path=\"DV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import mageai_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ./data/new_batches/prod_data_batch_1.parquet\n",
      "Predicting...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "training data did not have the following fields: birth_country=0, birth_country=1, birth_country_father=0, birth_country_father=1, birth_country_mother=0, birth_country_mother=1, chief_earner=1, chief_earner=2, children_number, citizenship=1, citizenship=2, country=104, country=124, country=152, country=156, country=158, country=170, country=196, country=20, country=203, country=218, country=231, country=276, country=300, country=32, country=320, country=344, country=36, country=360, country=364, country=368, country=392, country=398, country=400, country=404, country=410, country=417, country=422, country=434, country=446, country=458, country=462, country=484, country=496, country=50, country=51, country=68, country=76, education, education_mother, education_spouse, employment=1, employment=2, employment=3, employment=4, employment=5, employment=6, employment=7, employment_sector=0, employment_sector=1, employment_sector=2, employment_sector=3, employment_spouse=0, employment_spouse=1, employment_spouse=2, employment_spouse=3, employment_spouse=4, employment_spouse=5, employment_spouse=6, employment_spouse=7, employment_spouse=8, generation, household_size, immigrant=1, immigrant=2, immigrant_father=1, immigrant_father=2, immigrant_mother=1, immigrant_mother=2, income_scale, intprivacy=1, intprivacy=2, lives_with_parents=1, lives_with_parents=2, lives_with_parents=3, lives_with_parents=4, marital_status=1, marital_status=2, marital_status=3, marital_status=4, marital_status=5, marital_status=6, mode=1, mode=2, mode=3, mode=4, profession=1, profession=10, profession=11, profession=12, profession=2, profession=3, profession=4, profession=5, profession=6, profession=7, profession=8, profession=9, profession_father=1, profession_father=10, profession_father=11, profession_father=12, profession_father=2, profession_father=3, profession_father=4, profession_father=5, profession_father=6, profession_father=7, profession_father=8, profession_father=9, profession_spouse=0, profession_spouse=1, profession_spouse=10, profession_spouse=11, profession_spouse=12, profession_spouse=2, profession_spouse=3, profession_spouse=4, profession_spouse=5, profession_spouse=6, profession_spouse=7, profession_spouse=8, profession_spouse=9, religion=1, religion=10, religion=2, religion=3, religion=4, religion=5, religion=6, religion=7, religion=8, religion=9, respint, savings=1, savings=2, savings=3, savings=4, settlement=1, settlement=2, settlement=3, settlement=4, settlement=5, sex=1, sex=2, subjective_social_class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mmageai_supports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./mlflow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/new_batches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/political-engagement-mlops/misc/mageai_supports.py:68\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(artifacts_path, new_batches_folder_path, predictions_output_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m df_dmat \u001b[38;5;241m=\u001b[39m get_dmatrix(df, dv)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# predict (0=subject doesn't need intervention (-), 1=subject could benefit from intervention (CONTACT))\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_dmat\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     69\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mprediction\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTACT\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make things easier for the agents by only keeping the target subjects\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# moreover, keep only essential contact info\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.10/site-packages/xgboost/core.py:2359\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[1;32m   2358\u001b[0m     fn \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_names\n\u001b[0;32m-> 2359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2360\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2361\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: training,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m: strict_shape,\n\u001b[1;32m   2366\u001b[0m }\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign_type\u001b[39m(t: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlops/lib/python3.10/site-packages/xgboost/core.py:3047\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   3046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3047\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3049\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names)\n\u001b[1;32m   3050\u001b[0m     )\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m!=\u001b[39m feature_names:\n\u001b[1;32m   3053\u001b[0m     dat_missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(cast(FeatureNames, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m   3054\u001b[0m         cast(FeatureNames, feature_names)\n\u001b[1;32m   3055\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: training data did not have the following fields: birth_country=0, birth_country=1, birth_country_father=0, birth_country_father=1, birth_country_mother=0, birth_country_mother=1, chief_earner=1, chief_earner=2, children_number, citizenship=1, citizenship=2, country=104, country=124, country=152, country=156, country=158, country=170, country=196, country=20, country=203, country=218, country=231, country=276, country=300, country=32, country=320, country=344, country=36, country=360, country=364, country=368, country=392, country=398, country=400, country=404, country=410, country=417, country=422, country=434, country=446, country=458, country=462, country=484, country=496, country=50, country=51, country=68, country=76, education, education_mother, education_spouse, employment=1, employment=2, employment=3, employment=4, employment=5, employment=6, employment=7, employment_sector=0, employment_sector=1, employment_sector=2, employment_sector=3, employment_spouse=0, employment_spouse=1, employment_spouse=2, employment_spouse=3, employment_spouse=4, employment_spouse=5, employment_spouse=6, employment_spouse=7, employment_spouse=8, generation, household_size, immigrant=1, immigrant=2, immigrant_father=1, immigrant_father=2, immigrant_mother=1, immigrant_mother=2, income_scale, intprivacy=1, intprivacy=2, lives_with_parents=1, lives_with_parents=2, lives_with_parents=3, lives_with_parents=4, marital_status=1, marital_status=2, marital_status=3, marital_status=4, marital_status=5, marital_status=6, mode=1, mode=2, mode=3, mode=4, profession=1, profession=10, profession=11, profession=12, profession=2, profession=3, profession=4, profession=5, profession=6, profession=7, profession=8, profession=9, profession_father=1, profession_father=10, profession_father=11, profession_father=12, profession_father=2, profession_father=3, profession_father=4, profession_father=5, profession_father=6, profession_father=7, profession_father=8, profession_father=9, profession_spouse=0, profession_spouse=1, profession_spouse=10, profession_spouse=11, profession_spouse=12, profession_spouse=2, profession_spouse=3, profession_spouse=4, profession_spouse=5, profession_spouse=6, profession_spouse=7, profession_spouse=8, profession_spouse=9, religion=1, religion=10, religion=2, religion=3, religion=4, religion=5, religion=6, religion=7, religion=8, religion=9, respint, savings=1, savings=2, savings=3, savings=4, settlement=1, settlement=2, settlement=3, settlement=4, settlement=5, sex=1, sex=2, subjective_social_class"
     ]
    }
   ],
   "source": [
    "batch = mageai_supports.predict(\n",
    "        \"./mlflow\",\n",
    "        \"./data/new_batches\",\n",
    "        \"./data/predictions\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1 = pd.read_parquet(\"./data/batches/prod_data_batch_1.parquet\")\n",
    "b1 = pd.read_parquet(\"./data/new_batches/prod_data_batch_1.parquet\")\n",
    "n1 = pd.read_parquet(\"./data/batches/new_data_batch_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1.drop(columns=\"subject_id\", inplace=True)\n",
    "b = mageai_supports.get_dmatrix(b1, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1[\"pred\"] = booster.predict(b)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
